{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n",
      "b'Hi TensorFlow!!'\n"
     ]
    }
   ],
   "source": [
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "hi = tf.constant(\"Hi TensorFlow!!\")\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(hello))\n",
    "print(sess.run(hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1 :  Tensor(\"Const_38:0\", shape=(), dtype=float32) node2 :  Tensor(\"Const_39:0\", shape=(), dtype=float32)\n",
      "node3 :  Tensor(\"Add_3:0\", shape=(), dtype=float32)\n",
      "node 4:  Tensor(\"Const_40:0\", shape=(), dtype=float32)\n",
      "node6 :  Tensor(\"Sub_2:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0)\n",
    "node3 = tf.add(node1, node2)\n",
    "node4 = tf.constant(4.5)\n",
    "node5 = tf.constant(7.9)\n",
    "node6 = tf.subtract(node5,node4)\n",
    "\n",
    "print (\"node1 : \", node1, \"node2 : \", node2)\n",
    "print(\"node3 : \", node3)\n",
    "print(\"node 4: \", node4)\n",
    "print(\"node6 : \", node6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess.run(node1, node2) :  [3.0, 4.0]\n",
      "sess.run(node3) :  7.0\n",
      "sess.run(node4) :  4.5\n",
      "sess.run(node6) :  3.4\n"
     ]
    }
   ],
   "source": [
    "print(\"sess.run(node1, node2) : \", sess.run([node1,node2]))\n",
    "print(\"sess.run(node3) : \", sess.run(node3))\n",
    "print(\"sess.run(node4) : \", sess.run(node4))\n",
    "print(\"sess.run(node6) : \", sess.run(node6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.  7.]\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "c = tf.placeholder(tf.float32)\n",
    "adder_node = a+b\n",
    "subtract_node = b-c\n",
    "\n",
    "print (sess.run(adder_node, feed_dict = {a: 3, b: 4.5}))\n",
    "print ( sess.run(adder_node, feed_dict = {a: [1,3,4], b:[2,4,3]}))\n",
    "print (sess.run(subtract_node, feed_dict= {b: 4, c: 5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34  3  4]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([34,3,4])\n",
    "print(sess.run(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[1,2,3],[4,5,6]])\n",
    "print(sess.run(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 3]\n",
      "  [3 4 5]]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[[1,2,3],[3,4,5]]])\n",
    "print(sess.run(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ 0.03140762] [ 1.78111768]\n",
      "20 [ 2.50613284] [ 2.12976718]\n",
      "40 [ 2.70783377] [ 1.7340095]\n",
      "60 [ 2.84271264] [ 1.34255254]\n",
      "80 [ 2.9683156] [ 0.97339582]\n",
      "100 [ 3.08657146] [ 0.62571359]\n",
      "120 [ 3.19794273] [ 0.29826769]\n",
      "140 [ 3.30283213] [-0.01011908]\n",
      "160 [ 3.4016161] [-0.30055615]\n",
      "180 [ 3.49465013] [-0.57408816]\n",
      "200 [ 3.58226943] [-0.83169925]\n",
      "220 [ 3.66478872] [-1.07431602]\n",
      "240 [ 3.74250484] [-1.30281103]\n",
      "260 [ 3.81569743] [-1.5180068]\n",
      "280 [ 3.8846302] [-1.7206769]\n",
      "300 [ 3.94955039] [-1.91155052]\n",
      "320 [ 4.01069212] [-2.09131408]\n",
      "340 [ 4.06827497] [-2.26061463]\n",
      "360 [ 4.12250614] [-2.42006063]\n",
      "380 [ 4.17358065] [-2.57022619]\n",
      "400 [ 4.22168255] [-2.71165156]\n",
      "420 [ 4.26698446] [-2.84484529]\n",
      "440 [ 4.30964994] [-2.97028637]\n",
      "460 [ 4.34983158] [-3.08842635]\n",
      "480 [ 4.38767529] [-3.19968987]\n",
      "500 [ 4.423316] [-3.30447745]\n",
      "520 [ 4.45688152] [-3.40316606]\n",
      "540 [ 4.48849392] [-3.49610996]\n",
      "560 [ 4.51826668] [-3.58364415]\n",
      "580 [ 4.54630613] [-3.66608381]\n",
      "600 [ 4.5727129] [-3.74372458]\n",
      "620 [ 4.59758329] [-3.81684661]\n",
      "640 [ 4.62100601] [-3.88571167]\n",
      "660 [ 4.64306545] [-3.95056891]\n",
      "680 [ 4.66384077] [-4.01165104]\n",
      "700 [ 4.68340731] [-4.06917763]\n",
      "720 [ 4.7018342] [-4.12335682]\n",
      "740 [ 4.71918917] [-4.17438316]\n",
      "760 [ 4.73553467] [-4.22243929]\n",
      "780 [ 4.75092745] [-4.26769686]\n",
      "800 [ 4.76542521] [-4.31032038]\n",
      "820 [ 4.77907801] [-4.35046291]\n",
      "840 [ 4.7919364] [-4.38826942]\n",
      "860 [ 4.80404758] [-4.42387486]\n",
      "880 [ 4.81545305] [-4.45740843]\n",
      "900 [ 4.82619429] [-4.48898983]\n",
      "920 [ 4.83631039] [-4.51873302]\n",
      "940 [ 4.84583902] [-4.54674578]\n",
      "960 [ 4.85481119] [-4.57312727]\n",
      "980 [ 4.86326218] [-4.59797335]\n",
      "1000 [ 4.87122059] [-4.62137365]\n",
      "1020 [ 4.87871647] [-4.64341164]\n",
      "1040 [ 4.88577557] [-4.66416693]\n",
      "1060 [ 4.89242411] [-4.68371439]\n",
      "1080 [ 4.89868641] [-4.70212412]\n",
      "1100 [ 4.90458298] [-4.71946239]\n",
      "1120 [ 4.9101367] [-4.73579121]\n",
      "1140 [ 4.91536713] [-4.7511692]\n",
      "1160 [ 4.92029333] [-4.76565266]\n",
      "1180 [ 4.92493296] [-4.77929258]\n",
      "1200 [ 4.92930222] [-4.79213953]\n",
      "1220 [ 4.93341684] [-4.80423832]\n",
      "1240 [ 4.9372921] [-4.81563234]\n",
      "1260 [ 4.94094229] [-4.82636356]\n",
      "1280 [ 4.94437933] [-4.83647013]\n",
      "1300 [ 4.94761705] [-4.8459878]\n",
      "1320 [ 4.95066643] [-4.85495138]\n",
      "1340 [ 4.95353746] [-4.86339426]\n",
      "1360 [ 4.95624161] [-4.87134504]\n",
      "1380 [ 4.95878887] [-4.87883377]\n",
      "1400 [ 4.96118736] [-4.88588667]\n",
      "1420 [ 4.96344662] [-4.89252806]\n",
      "1440 [ 4.96557426] [-4.89878416]\n",
      "1460 [ 4.96757793] [-4.90467548]\n",
      "1480 [ 4.96946478] [-4.91022348]\n",
      "1500 [ 4.97124243] [-4.91544914]\n",
      "1520 [ 4.9729166] [-4.9203701]\n",
      "1540 [ 4.97449303] [-4.92500544]\n",
      "1560 [ 4.97597742] [-4.9293704]\n",
      "1580 [ 4.97737551] [-4.93348169]\n",
      "1600 [ 4.97869253] [-4.93735361]\n",
      "1620 [ 4.97993279] [-4.94099998]\n",
      "1640 [ 4.98110056] [-4.94443321]\n",
      "1660 [ 4.98220062] [-4.94766951]\n",
      "1680 [ 4.98323774] [-4.95071602]\n",
      "1700 [ 4.98421288] [-4.95358515]\n",
      "1720 [ 4.98513222] [-4.95628643]\n",
      "1740 [ 4.9859972] [-4.95883083]\n",
      "1760 [ 4.98681259] [-4.96122694]\n",
      "1780 [ 4.98757982] [-4.96348381]\n",
      "1800 [ 4.98830318] [-4.96560955]\n",
      "1820 [ 4.98898363] [-4.96761131]\n",
      "1840 [ 4.98962498] [-4.96949673]\n",
      "1860 [ 4.99022913] [-4.97127247]\n",
      "1880 [ 4.99079752] [-4.97294426]\n",
      "1900 [ 4.99133301] [-4.97451782]\n",
      "1920 [ 4.9918375] [-4.97600126]\n",
      "1940 [ 4.99231243] [-4.97739792]\n",
      "1960 [ 4.99276018] [-4.97871351]\n",
      "1980 [ 4.99318123] [-4.97995234]\n",
      "2000 [ 4.99357796] [-4.98111868]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([1,2,3,4])\n",
    "y_train = np.array([1,4,9,16])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "hypothesis = x_train * W + b\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(2001):\n",
    "    sess.run(train)\n",
    "    if step%20 ==0:\n",
    "        print (step, sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15.2696 0.81 0.94 2.978\n",
      "20 4.82035 0.542391 0.747304 2.84584\n",
      "40 4.2908 0.579231 0.647845 2.74605\n",
      "60 3.81991 0.613972 0.554142 2.65186\n",
      "80 3.40117 0.646718 0.465863 2.56296\n",
      "100 3.02881 0.677581 0.382697 2.47904\n",
      "120 2.69769 0.706668 0.304354 2.39981\n",
      "140 2.40324 0.734083 0.230558 2.32501\n",
      "160 2.1414 0.759919 0.16105 2.25439\n",
      "180 1.90856 0.784266 0.0955851 2.18771\n",
      "200 1.7015 0.80721 0.0339327 2.12475\n",
      "220 1.51737 0.828831 -0.0241247 2.06529\n",
      "240 1.35363 0.849204 -0.0787922 2.00913\n",
      "260 1.20801 0.8684 -0.130263 1.95608\n",
      "280 1.07852 0.886487 -0.17872 1.90597\n",
      "300 0.963354 0.903527 -0.224335 1.85864\n",
      "320 0.860939 0.919581 -0.267269 1.81391\n",
      "340 0.769857 0.934705 -0.307677 1.77165\n",
      "360 0.688855 0.948951 -0.345702 1.73171\n",
      "380 0.616814 0.96237 -0.381479 1.69396\n",
      "400 0.552742 0.975009 -0.415139 1.65828\n",
      "420 0.495757 0.986912 -0.4468 1.62455\n",
      "440 0.445073 0.998122 -0.476578 1.59265\n",
      "460 0.399992 1.00868 -0.504579 1.56249\n",
      "480 0.359894 1.01862 -0.530906 1.53396\n",
      "500 0.324226 1.02797 -0.555654 1.50697\n",
      "520 0.292498 1.03678 -0.578912 1.48143\n",
      "540 0.264273 1.04508 -0.600766 1.45727\n",
      "560 0.239163 1.05288 -0.621297 1.4344\n",
      "580 0.216823 1.06023 -0.640579 1.41274\n",
      "600 0.196946 1.06714 -0.658685 1.39224\n",
      "620 0.17926 1.07364 -0.67568 1.37282\n",
      "640 0.163521 1.07976 -0.691629 1.35442\n",
      "660 0.149515 1.08551 -0.706592 1.33699\n",
      "680 0.137048 1.09092 -0.720625 1.32046\n",
      "700 0.125952 1.09601 -0.73378 1.3048\n",
      "720 0.116072 1.10079 -0.746108 1.28994\n",
      "740 0.107276 1.10529 -0.757657 1.27584\n",
      "760 0.0994432 1.10952 -0.76847 1.26247\n",
      "780 0.0924665 1.11349 -0.778591 1.24977\n",
      "800 0.0862512 1.11721 -0.788058 1.23771\n",
      "820 0.0807131 1.12072 -0.796908 1.22626\n",
      "840 0.0757772 1.124 -0.805178 1.21538\n",
      "860 0.0713767 1.12709 -0.8129 1.20503\n",
      "880 0.0674525 1.12999 -0.820106 1.19519\n",
      "900 0.0639517 1.1327 -0.826825 1.18582\n",
      "920 0.0608274 1.13525 -0.833085 1.17691\n",
      "940 0.0580382 1.13764 -0.838913 1.16842\n",
      "960 0.0555467 1.13987 -0.844333 1.16034\n",
      "980 0.0533202 1.14197 -0.849368 1.15263\n",
      "1000 0.0513292 1.14393 -0.854041 1.14528\n",
      "1020 0.0495476 1.14576 -0.858372 1.13826\n",
      "1040 0.0479525 1.14748 -0.862381 1.13156\n",
      "1060 0.0465229 1.14909 -0.866087 1.12517\n",
      "1080 0.0452408 1.15058 -0.869506 1.11905\n",
      "1100 0.0440898 1.15198 -0.872655 1.1132\n",
      "1120 0.0430553 1.15329 -0.875551 1.1076\n",
      "1140 0.0421246 1.1545 -0.878206 1.10224\n",
      "1160 0.041286 1.15564 -0.880636 1.09711\n",
      "1180 0.0405296 1.15669 -0.882852 1.09218\n",
      "1200 0.0398461 1.15767 -0.884868 1.08745\n",
      "1220 0.0392276 1.15858 -0.886695 1.08291\n",
      "1240 0.0386669 1.15943 -0.888344 1.07855\n",
      "1260 0.0381575 1.16021 -0.889824 1.07436\n",
      "1280 0.037694 1.16093 -0.891147 1.07032\n",
      "1300 0.0372711 1.1616 -0.89232 1.06644\n",
      "1320 0.0368844 1.16222 -0.893352 1.06269\n",
      "1340 0.0365299 1.16278 -0.894252 1.05908\n",
      "1360 0.0362042 1.16331 -0.895028 1.05559\n",
      "1380 0.0359041 1.16378 -0.895686 1.05222\n",
      "1400 0.0356266 1.16422 -0.896233 1.04896\n",
      "1420 0.0353694 1.16462 -0.896676 1.04581\n",
      "1440 0.0351303 1.16498 -0.89702 1.04275\n",
      "1460 0.0349072 1.1653 -0.897272 1.03979\n",
      "1480 0.0346985 1.1656 -0.897437 1.03692\n",
      "1500 0.0345024 1.16586 -0.897519 1.03414\n",
      "1520 0.0343179 1.1661 -0.897525 1.03143\n",
      "1540 0.0341434 1.16631 -0.897457 1.02879\n",
      "1560 0.0339781 1.16649 -0.897321 1.02623\n",
      "1580 0.0338208 1.16665 -0.897121 1.02374\n",
      "1600 0.0336707 1.16678 -0.896859 1.0213\n",
      "1620 0.033527 1.1669 -0.89654 1.01893\n",
      "1640 0.0333892 1.16699 -0.896168 1.01661\n",
      "1660 0.0332564 1.16706 -0.895745 1.01435\n",
      "1680 0.0331283 1.16712 -0.895274 1.01213\n",
      "1700 0.0330043 1.16716 -0.894759 1.00997\n",
      "1720 0.032884 1.16719 -0.894201 1.00785\n",
      "1740 0.0327671 1.1672 -0.893603 1.00577\n",
      "1760 0.032653 1.16719 -0.892968 1.00373\n",
      "1780 0.0325416 1.16717 -0.892298 1.00173\n",
      "1800 0.0324327 1.16714 -0.891595 0.99976\n",
      "1820 0.0323258 1.1671 -0.890861 0.997828\n",
      "1840 0.0322211 1.16705 -0.890098 0.995929\n",
      "1860 0.032118 1.16698 -0.889308 0.99406\n",
      "1880 0.0320167 1.16691 -0.888492 0.992219\n",
      "1900 0.0319167 1.16683 -0.887651 0.990406\n",
      "1920 0.0318179 1.16674 -0.886788 0.988619\n",
      "1940 0.0317205 1.16664 -0.885904 0.986856\n",
      "1960 0.0316242 1.16653 -0.885 0.985116\n",
      "1980 0.0315287 1.16642 -0.884077 0.983398\n",
      "2000 0.0314341 1.1663 -0.883136 0.981701\n",
      "1.1663 -0.883136 0.981701\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([1,2,3,4])\n",
    "y_train = np.array([1,4,9,16])\n",
    "x_sq = x_train**2\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable((1.), name='weight')\n",
    "b = tf.Variable((3.),name=\"BIAS\")\n",
    "W_2 = tf.Variable((1.), name=\"weight_x2\")\n",
    "\n",
    "hypothesis = X**2 *W_2 + X * W + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.002)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "for step in range(2001):\n",
    "    #cost_val, W_val, b_val, _ = sess.run([cost, W, b, train], feed_dict={X: x_train, Y: y_train})\n",
    "    sess.run(train, feed_dict={X: x_train, Y: y_train})\n",
    "    if step%20 == 0:\n",
    "        print(step, sess.run(cost, feed_dict={X: x_train, Y: y_train}), sess.run(W_2), sess.run(W),sess.run(b))#, cost_val,  W_val, b_val )\n",
    "    if step == 2000:\n",
    "        fin_w2 = sess.run(W_2)\n",
    "        fin_w = sess.run(W)\n",
    "        fin_b = sess.run(b)\n",
    "        print(fin_w2, fin_w, fin_b)\n",
    "        \n",
    "# 두가지 버전이 존재한다 플레이스 홀더를 썼다고  train 만 따로 돌릴 수 없는 것은 아니다. 다만~! \n",
    "# 위와 같이 세스 런을 맞춰준다면, 쉽게 구할 수 있다. \n",
    "# 옵티마이저가 바꿔주는 것은 베리어블이고, 베리어블과 상관 없이 코스트 함수또한 존재하는 베리어블 값만 불러오면 계산 할 수가 있다. 만약 다른걸 너어주면.. \n",
    "#값이 바뀌곘지만. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnCDQASAn/h+MGUU8mkNFDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119f389e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.linspace(0,5,num=100)\n",
    "plt.scatter(xx,(xx**2*fin_w2 + xx*fin_w + fin_b))\n",
    "plt.scatter(x_train,y_train,c=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
